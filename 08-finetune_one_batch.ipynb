{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Intro\n",
        "In this notebook's code we will fine-tune a transformer model based on a very small batch of data."
      ],
      "metadata": {
        "id": "7Fj1SdBTk9aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load tokenizer and model\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "# Data\n",
        "raw_data = [\n",
        "    \"There may be too many of them!\",\n",
        "    \"You will not believe this\",\n",
        "    \"Hey man! That is not cool at all.\"\n",
        "]\n",
        "batch = tokenizer(raw_data, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Set the targets\n",
        "batch[\"labels\"] = torch.tensor([0, 1, 0])\n",
        "\n",
        "# Now let us predict same examples before the update\n",
        "with torch.no_grad():\n",
        "    outputs = torch.nn.functional.softmax(model(**batch).logits, dim=-1)\n",
        "\n",
        "print(outputs)\n",
        "\n",
        "# Fine-tune\n",
        "optimizer = AdamW(model.parameters())\n",
        "loss = model(**batch).loss\n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zax-TEYYKQa",
        "outputId": "ba4aee98-f556-4d94-b21d-88b191d4e1ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5432, 0.4568],\n",
            "        [0.5321, 0.4679],\n",
            "        [0.5821, 0.4179]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us predict the same examples:"
      ],
      "metadata": {
        "id": "pukrjL1GctlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let us predict the same examples\n",
        "with torch.no_grad():\n",
        "    outputs = torch.nn.functional.softmax(model(**batch).logits, dim=-1)\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "KBe9khYhU1dW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947505aa-9ba3-4234-88e9-6df16e0a9927"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9155, 0.0845],\n",
            "        [0.9383, 0.0617],\n",
            "        [0.8877, 0.1123]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMK4hQ9CfS0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}