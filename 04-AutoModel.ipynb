{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Next in the process of a `pipeline` we have the `Automodel`.\n",
        "\n",
        "It works similar to `AutoTokenizer` in that it detects the model automatically."
      ],
      "metadata": {
        "id": "7Fj1SdBTk9aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# First tokenize\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "rawtext = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "tokens = tokenizer(rawtext, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(\"Tokens: \\n\", tokens)\n",
        "\n",
        "# Now the model\n",
        "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "outputs = model(**tokens)\n",
        "print(\"Outputs: \\n\", outputs)\n",
        "print(\"Hidden State Shape: \", outputs.last_hidden_state.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zax-TEYYKQa",
        "outputId": "54ff6889-c540-4f68-feb1-e34ca001fa8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: \n",
            " {'input_ids': tensor([[  101,   146,   112,  1396,  1151,  2613,  1111,   170, 20164, 10932,\n",
            "          2271,  7954,  1736,  1139,  2006,  1297,   119,   102],\n",
            "        [  101,   146,  4819,  1142,  1177,  1277,   106,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "Outputs: \n",
            " BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.4222,  0.4443, -0.0659,  ..., -0.1958,  0.3611,  0.1284],\n",
            "         [ 0.5728, -0.1593,  0.6014,  ..., -0.1134,  0.1791,  0.1787],\n",
            "         [ 0.4699,  0.4214,  0.1695,  ...,  0.2386,  0.9851, -0.1236],\n",
            "         ...,\n",
            "         [ 0.5847,  0.2552,  0.0266,  ...,  0.7203,  0.0650,  0.4277],\n",
            "         [ 0.5573,  0.4506,  0.0353,  ..., -0.0607,  0.4209, -0.2525],\n",
            "         [ 0.7136,  1.2932, -0.2937,  ...,  0.2917,  0.4270, -0.3874]],\n",
            "\n",
            "        [[ 0.4829,  0.4291,  0.0264,  ..., -0.1489,  0.2953, -0.3113],\n",
            "         [ 0.2735,  0.4520,  0.2760,  ...,  0.2572,  0.2059,  0.4097],\n",
            "         [ 0.1391,  0.4234, -0.3385,  ...,  0.5858, -0.0834,  0.4344],\n",
            "         ...,\n",
            "         [ 0.0709,  0.4650, -0.1060,  ...,  0.2954,  0.1990,  0.1774],\n",
            "         [ 0.1649,  0.4855, -0.0801,  ...,  0.3485,  0.1970,  0.1701],\n",
            "         [ 0.2887,  0.4945,  0.0196,  ...,  0.2895,  0.2056,  0.0399]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6911,  0.4541,  0.9999,  ...,  1.0000, -0.8468,  0.9912],\n",
            "        [-0.5548,  0.4425,  0.9998,  ...,  1.0000, -0.9216,  0.9935]],\n",
            "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
            "Output Shape:  torch.Size([2, 18, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here hidden state size is $[2, 18, 768]$, since 2 is the batch size (number of examples), 18 is the max token number that we have padded the examples to, and 768 is the hidden size or the dimension of the contextualized embedding.\n",
        "\n",
        "Now let us convert the results back to human intrepretable results:"
      ],
      "metadata": {
        "id": "hMt7L9y5pnBz"
      }
    }
  ]
}