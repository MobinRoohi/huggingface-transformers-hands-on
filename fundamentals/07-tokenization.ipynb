{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Intro\n",
        "Let us take a good look at the two steps governing tokenization."
      ],
      "metadata": {
        "id": "7Fj1SdBTk9aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Load pre-trained model\n",
        "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# Some data to work with\n",
        "raw_data = [\n",
        "    \"There may be too many of them, Mobin!\",\n",
        "    \"You will not believe this\",\n",
        "    \"Hey man! That is not cool at all.\"\n",
        "]\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# Usual tokenization\n",
        "inputs = tokenizer(raw_data, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zax-TEYYKQa",
        "outputId": "c694ea2c-136f-4429-b247-fd96debb4bbe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1247,  1336,  1129,  1315,  1242,  1104,  1172,   117, 12556,\n",
              "          7939,   106,   102],\n",
              "        [  101,  1192,  1209,  1136,  2059,  1142,   102,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [  101,  4403,  1299,   106,  1337,  1110,  1136,  4348,  1120,  1155,\n",
              "           119,   102,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now imagine that we fine-tuned the model and now want to save it."
      ],
      "metadata": {
        "id": "EJPqVeJZU6hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will dissect this into tokenization and encoding\n",
        "# Tokenize into subwords\n",
        "tokenized_data = tokenizer.tokenize(raw_data[0])\n",
        "tokenized_data"
      ],
      "metadata": {
        "id": "KBe9khYhU1dW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93165ee-2f75-4581-fd89-62d959243aaa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['There', 'may', 'be', 'too', 'many', 'of', 'them', ',', 'Mo', '##bin', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding\n",
        "ids = tokenizer.convert_tokens_to_ids(tokenized_data)\n",
        "\n",
        "# The other way around would be:\n",
        "# tokens = tokenizer.convert_ids_to_tokens(ids)\n",
        "\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXSjhzA2Djgt",
        "outputId": "bb7349e8-cf4e-41f0-f2ad-5ca6dd1c8e8c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1247, 1336, 1129, 1315, 1242, 1104, 1172, 117, 12556, 7939, 106]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only difference heere is that the start and end of the sentence tokens are missing.\n",
        "\n",
        "Now let us decode:"
      ],
      "metadata": {
        "id": "DIkRqf3_SQZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_tokens = tokenizer.decode(ids)\n",
        "decoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BtsjnuvVDknW",
        "outputId": "46bfde28-faad-4139-b16d-3c15a7989e5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There may be too many of them, Mobin!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the config file represents the configurations of the model while the model.safetensors file represents the model parameters and weights."
      ],
      "metadata": {
        "id": "K-1NE5s9EBDo"
      }
    }
  ]
}