{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fj1SdBTk9aH"
      },
      "source": [
        "## A Deeper Dive into Tokenizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zax-TEYYKQa",
        "outputId": "3208a71c-8a23-4fb6-9f7c-68e17d55c3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "{'input_ids': [101, 1422, 1271, 1110, 156, 7777, 2497, 1394, 1105, 146, 1250, 1120, 20164, 10932, 10289, 1107, 6010, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "encoding = tokenizer(example)\n",
        "print(type(encoding))\n",
        "print(encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We choose `whole_func_string` feature of the dataset to train the tokenizer on."
      ],
      "metadata": {
        "id": "eE4MFJIjKiXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the tokenizer is fast (code written in Rust to parallelize and be faster) or not\n",
        "tokenizer.is_fast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XnnLhBMQLjW",
        "outputId": "e4fbe52a-9006-4c88-c48d-c07eafaf28a8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "935etI7R0kLE",
        "outputId": "85b284d9-488f-4a8c-8ac3-5ca0cb417094"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'S',\n",
              " '##yl',\n",
              " '##va',\n",
              " '##in',\n",
              " 'and',\n",
              " 'I',\n",
              " 'work',\n",
              " 'at',\n",
              " 'Hu',\n",
              " '##gging',\n",
              " 'Face',\n",
              " 'in',\n",
              " 'Brooklyn',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "encoding.tokens()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.word_ids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMXFJnFTQexC",
        "outputId": "3843032f-101c-4b50-8f41-c1fd918be059"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The notion of what a word is complicated. For instance, does “I’ll” (a contraction of “I will”) count as one or two words? It actually depends on the tokenizer and the pre-tokenization operation it applies. Some tokenizers just split on spaces, so they will consider this as one word. Others use punctuation on top of spaces, so will consider it two words.\n",
        "\n",
        "To see some examples, we will a tokenizer from the bert-base-cased and roberta-base checkpoints and tokenize ”81s” with them."
      ],
      "metadata": {
        "id": "xoEADacJQ6o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"81s\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "bert = bert_tokenizer(example)\n",
        "roberta = roberta_tokenizer(example)\n",
        "print(bert.tokens(), roberta.tokens())\n",
        "print(bert.word_ids(), roberta.word_ids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uhIz2AyQpFh",
        "outputId": "d96ec189-fa94-4fdc-f349-899454db6e0c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '81', '##s', '[SEP]'] ['<s>', '81', 's', '</s>']\n",
            "[None, 0, 0, None] [None, 0, 1, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thererfore, the BERT tokenizer considers 81s as a single word, while RoBERTa consideres it as two words."
      ],
      "metadata": {
        "id": "5yWXO8CbRmJP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "69RAI5pBRMCM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}