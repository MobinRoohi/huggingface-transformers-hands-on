{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Intro\n",
        "In this notebook, we will go through each of the steps taken behind `pipline` separately and then compare the results to ensure that the step were correctly taken."
      ],
      "metadata": {
        "id": "7Fj1SdBTk9aH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "p4DXqXRMU3RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Raw input\n",
        "raw_inputs = [\n",
        "    \"These days the weather has been a little too hot.\",\n",
        "    \"I really want this for you, girl!\",\n",
        "    \"You'd better not bring her into this!\"\n",
        "]\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zax-TEYYKQa",
        "outputId": "8caf796f-54e5-4f8a-eedb-05c914256186"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 2122, 2420, 1996, 4633, 2038, 2042, 1037, 2210, 2205, 2980, 1012,\n",
              "          102],\n",
              "        [ 101, 1045, 2428, 2215, 2023, 2005, 2017, 1010, 2611,  999,  102,    0,\n",
              "            0],\n",
              "        [ 101, 2017, 1005, 1040, 2488, 2025, 3288, 2014, 2046, 2023,  999,  102,\n",
              "            0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n"
      ],
      "metadata": {
        "id": "EJPqVeJZU6hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load model from checkpoint\n",
        "classifier = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "# Estimate logits\n",
        "outputs = classifier(**inputs)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBe9khYhU1dW",
        "outputId": "7fa51e27-1d55-44fd-bd9d-22b1c3f20fa4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0402, -3.3439],\n",
              "        [-4.0005,  4.2854],\n",
              "        [ 3.7932, -3.1592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post Processing"
      ],
      "metadata": {
        "id": "TtG-GVd8Vgw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Convert logits to probabilities\n",
        "predictions = softmax(outputs.logits, dim=-1)\n",
        "\n",
        "# Find the labels\n",
        "print(classifier.config.id2label)\n",
        "\n",
        "for i in range(predictions.shape[0]):\n",
        "  print([item.item() for item in predictions[i]])\n",
        "\n",
        "for i in predictions.argmax(axis=1):\n",
        "  print(classifier.config.id2label[i.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsRm70GAVag0",
        "outputId": "7faff548-ee40-4eeb-8400-bf582c892b4b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'NEGATIVE', 1: 'POSITIVE'}\n",
            "[0.9993792772293091, 0.0006206354591995478]\n",
            "[0.0002519783447496593, 0.9997480511665344]\n",
            "[0.9990445971488953, 0.0009554324205964804]\n",
            "NEGATIVE\n",
            "POSITIVE\n",
            "NEGATIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare with Pipline"
      ],
      "metadata": {
        "id": "7auYPOEmXPcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# pipeline\n",
        "pipeline = pipeline(\"sentiment-analysis\", model=checkpoint)\n",
        "\n",
        "# predict\n",
        "pipeline(raw_inputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjc3ofWMWj7S",
        "outputId": "25481925-5642-49cc-8017-94e3e9fe31c8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9993792772293091},\n",
              " {'label': 'POSITIVE', 'score': 0.9997480511665344},\n",
              " {'label': 'NEGATIVE', 'score': 0.9990445971488953}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matching the values we can see that our step-by-step implementation of `pipeline`, matches up well with using the high-level API call."
      ],
      "metadata": {
        "id": "GNAwyBX4Y1mn"
      }
    }
  ]
}